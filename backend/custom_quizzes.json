[{"id": "quiz_1_20250826112926", "title": "Interview", "description": "AI-generated quiz on Data Visualization", "difficulty": "hard", "duration": 10, "questions": [{"question": "Which data visualization technique is most effective for displaying hierarchical data with numerous levels and complex relationships?", "options": ["Stacked Bar Chart", "Heatmap", "Sankey Diagram", "Scatter Plot"], "correct_answer": 2, "explanation": "Sankey diagrams excel at visualizing flows and their magnitudes within a hierarchy, making them suitable for complex hierarchical data.", "type": "multiple_choice"}, {"question": "In the context of data visualization, what is the primary purpose of 'small multiples'?", "options": ["To show the correlation between different variables.", "To compare trends across different categories or time periods.", "To represent the overall distribution of a single variable.", "To highlight outliers in the dataset."], "correct_answer": 1, "explanation": "Small multiples (e.g., using facets or trellis charts) allow for comparing similar visualisations across different slices of your data, enabling comparison of trends.", "type": "multiple_choice"}, {"question": "What is the principle behind 'Gestalt principles' in data visualization, and how are they applied?", "options": ["They describe the optimal data encoding based on a specific statistical methodology.", "They are aesthetic guidelines to create visually appealing charts.", "They exploit how the human eye naturally perceives visual elements to improve the clarity and interpretability of data visualizations.", "They define the color palettes used in specific types of plots."], "correct_answer": 2, "explanation": "Gestalt principles (like proximity, similarity, closure) guide how our brains group and interpret visual elements, used to highlight relationships in the data.", "type": "multiple_choice"}, {"question": "Which type of chart is *least* suitable for visualizing the distribution of a large dataset with significant skewness?", "options": ["Box Plot", "Histogram", "Density Plot", "Pie Chart"], "correct_answer": 3, "explanation": "Pie charts are generally ineffective at conveying distributions, and are particularly bad when visualizing data that is not evenly distributed.", "type": "multiple_choice"}, {"question": "What is the potential risk of using a logarithmic scale on the y-axis in a chart?", "options": ["It can exaggerate small differences between values.", "It can make it difficult to accurately interpret the overall trend, especially for viewers unfamiliar with log scales.", "It always hides any values that are less than zero.", "It increases the chart's readability for datasets with extreme values."], "correct_answer": 1, "explanation": "While logarithmic scales can improve readability for datasets with large ranges, they can obscure the true magnitude of differences, particularly for those not familiar with their properties.", "type": "multiple_choice"}], "created_at": "2025-08-26T11:29:26.123166", "user_id": null}, {"id": "quiz_2_20250826114235", "title": "Interview prep", "description": "AI-generated quiz on Data Engineering", "difficulty": "hard", "duration": 16, "questions": [{"question": "Which of the following is the most appropriate technique for handling slowly changing dimensions (SCDs) in a data warehouse when tracking both the current and historical values of an attribute?", "options": ["Type 0: No Changes", "Type 1: Overwrite", "Type 2: Add New Row", "Type 3: Historical Attributes"], "correct_answer": 2, "explanation": "Type 2 SCDs, or adding a new row, is the most common approach for preserving the history of attribute changes by creating a new record for each change, thus allowing tracking over time.", "type": "multiple_choice"}, {"question": "In the context of distributed systems, what is the primary purpose of a consensus algorithm like Paxos or Raft?", "options": ["To optimize query performance across distributed data stores", "To ensure data consistency and fault tolerance among distributed nodes", "To load balance incoming requests across multiple servers", "To provide a centralized authentication mechanism for the system"], "correct_answer": 1, "explanation": "Consensus algorithms are crucial for maintaining consistency and reliability in distributed systems. They allow nodes to agree on a single value or state, even in the presence of failures.", "type": "multiple_choice"}, {"question": "What is the key difference between a data lake and a data warehouse in terms of data schema?", "options": ["Data lakes enforce a strict schema-on-write, while data warehouses use schema-on-read.", "Data warehouses enforce a strict schema-on-write, while data lakes use schema-on-read.", "Data lakes use schema-on-write, while data warehouses do not use schemas.", "Data warehouses are schema-less, while data lakes enforce schema-on-read."], "correct_answer": 1, "explanation": "Data warehouses typically use a schema-on-write approach, where the schema is defined before data is loaded. Data lakes often use schema-on-read, allowing the schema to be applied at the time of querying.", "type": "multiple_choice"}, {"question": "What is the purpose of a metadata management system in a data engineering pipeline?", "options": ["To store the actual data files and tables used in the pipeline.", "To manage and track data lineage, data quality, and data governance information.", "To execute the ETL processes and data transformations.", "To provide end-users with a user interface for querying data."], "correct_answer": 1, "explanation": "Metadata management systems provide critical context about data within a pipeline, ensuring that data lineage, data quality, and governance requirements can be met. ", "type": "multiple_choice"}, {"question": "Which Apache project is commonly used for stream processing and is particularly well-suited for fault tolerance and exactly-once processing guarantees?", "options": ["Spark", "Kafka", "Flink", "Hadoop"], "correct_answer": 2, "explanation": "Apache Flink is specifically designed for stream processing and offers strong guarantees regarding fault tolerance and exactly-once processing, unlike some of the other alternatives, like Spark Streaming.", "type": "multiple_choice"}, {"question": "What is the primary benefit of using a data orchestration tool like Airflow or Prefect?", "options": ["To store large volumes of unstructured data.", "To automatically scale compute resources based on demand.", "To manage and schedule complex data pipelines, providing monitoring and alerting.", "To build interactive dashboards and visualizations for data exploration."], "correct_answer": 2, "explanation": "Data orchestration tools simplify the process of scheduling, monitoring, and managing the dependencies within complex data pipelines, including monitoring and alerting.", "type": "multiple_choice"}, {"question": "In the context of data warehousing, what is a star schema primarily used for?", "options": ["Storing unstructured data in a hierarchical format.", "Enabling efficient data aggregation and querying for analytical workloads.", "Implementing real-time data processing and streaming.", "Storing highly normalized data to minimize redundancy."], "correct_answer": 1, "explanation": "The star schema, composed of a fact table and dimension tables, is designed for optimal querying speed for analytical processing.", "type": "multiple_choice"}, {"question": "What does the term 'data lineage' refer to in data engineering?", "options": ["The physical location of data storage infrastructure.", "The process of cleaning and transforming raw data.", "The history and origin of data, including its transformations and movements.", "The visualization of data using charts and graphs."], "correct_answer": 2, "explanation": "Data lineage tracks the lifecycle of data, from its origin to its current state. This helps in understanding the impact of changes, debugging issues, and ensuring data quality.", "type": "multiple_choice"}], "created_at": "2025-08-26T11:42:35.997880", "user_id": null}, {"id": "quiz_3_20250828111627", "title": "ML interview Prep", "description": "AI-generated quiz on Machine Learning", "difficulty": "medium", "duration": 10, "questions": [{"question": "Which of the following is NOT a characteristic of a good loss function?", "options": ["It should be differentiable.", "It should be convex.", "It should have a global minimum.", "It should be complex and difficult to compute."], "correct_answer": 3, "explanation": "A good loss function should be easy to compute and understand. Complex functions are usually difficult to optimize.", "type": "multiple_choice"}, {"question": "What is the primary purpose of regularization in machine learning?", "options": ["To increase the training accuracy.", "To reduce overfitting.", "To improve the model's interpretability.", "To speed up the training process."], "correct_answer": 1, "explanation": "Regularization techniques, like L1 and L2 regularization, are designed to prevent overfitting by adding a penalty to complex models.", "type": "multiple_choice"}, {"question": "In a classification problem, what is the difference between precision and recall?", "options": ["Precision focuses on minimizing false positives, while recall focuses on minimizing false negatives.", "Precision focuses on minimizing false negatives, while recall focuses on minimizing false positives.", "Precision is a measure of the model's overall accuracy, while recall is a measure of its precision.", "Precision and recall are interchangeable and measure the same thing."], "correct_answer": 0, "explanation": "Precision is about the accuracy of positive predictions (few false positives), while recall is about finding all the positive instances (few false negatives).", "type": "multiple_choice"}, {"question": "Which of the following is a key advantage of using ensemble methods like Random Forests or Gradient Boosting?", "options": ["They are less prone to overfitting than single models.", "They are always easier to interpret than single models.", "They require less data to train effectively.", "They always train faster than individual models."], "correct_answer": 0, "explanation": "Ensemble methods combine multiple models, leading to more robust and less overfitting models, and often improve overall performance.", "type": "multiple_choice"}, {"question": "What is the purpose of dimensionality reduction techniques like Principal Component Analysis (PCA)?", "options": ["To increase the number of features.", "To reduce the number of features while preserving important information.", "To introduce more noise into the data.", "To make the data non-linearly separable."], "correct_answer": 1, "explanation": "PCA aims to reduce the number of features in a dataset while retaining as much variance (information) as possible.", "type": "multiple_choice"}], "created_at": "2025-08-28T11:16:27.742150", "user_id": null}]